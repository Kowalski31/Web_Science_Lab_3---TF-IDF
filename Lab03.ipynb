{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QACY3c_lr4uz"
      },
      "source": [
        "# Lab03: TF-IDF.\n",
        "\n",
        "- MSSV: 19120689\n",
        "- Họ và tên: Lại Khánh Toàn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eD8WblEMr4u1"
      },
      "source": [
        "## Yêu cầu bài tập\n",
        "\n",
        "**Cách làm bài**\n",
        "\n",
        "\n",
        "Bạn sẽ làm trực tiếp trên file notebook này; từ `TODO` cho biết những phần mà bạn cần phải làm.\n",
        "\n",
        "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
        "\n",
        "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
        "\n",
        "**Cách nộp bài**\n",
        "\n",
        "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
        "\n",
        "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file notebook, file `tf_idf_data.txt` của các bạn (nếu file này kích thước lớn các bạn có thể chép link vào `link_data.txt`), nén thư mục `MSSV` này lại và nộp trên moodle.\n",
        "\n",
        "**Nội dung bài tập**\n",
        "\n",
        "Cài đặt một web crawler để thu thập và biểu diễn dữ liệu bằng không gian vector và trọng số TF-IDF: https://en.wikipedia.org/wiki/Web_mining. Truy vấn dựa trên cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txFiZE0Zr4u2"
      },
      "source": [
        "## Nội dung bài tập"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pU505-86r4u4"
      },
      "source": [
        "## 1. Không gian vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DCPXdVrr4u4"
      },
      "source": [
        "- Một vector 2 chiều có thể được viết dưới dạng `[x,y]`\n",
        "- Một vector 3 chiều có thể được viết dưới dạng `[x,y,z]`\n",
        "![vector](vector.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tFgEzX5Xc9e6"
      },
      "source": [
        "- Đặt $V$ là tập hợp các đặc trưng trong thể hiện dữ liệu.\n",
        "- Bất kỳ mẫu dữ liệu nào đều có thể được biểu diễn dưới dạng một vectơ với $\\vert V\\vert$ chiều\n",
        "- Ví dụ: giả sử chúng ta có 3 đặc trưng là các từ dog, bite, man. Giá trị 1 thể hiện từ đó xuất hiện 1 lần, 0 là không xuất hiện.  \n",
        "![space](space.png)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6kgzYNRKr4u5"
      },
      "source": [
        "## 2. Thu thập và thể hiện dữ liệu\n",
        "Thu thập dữ liệu bắt đầu từ URL: https://en.wikipedia.org/wiki/Web_mining.\n",
        "- Gọi $D$ là một tập văn bản chứa *n* văn bản: $D=\\left\\{d_1,d_2,...,d_n\\right\\}$.\n",
        "- $V=\\left\\{v_1,v_2,...v_{\\vert V \\vert}\\right\\}$ là từ điển (tất cả các từ phân biệt trong dữ liệu thu được). $\\vert V \\vert$ là kích thước từ điển.\n",
        "- Một trọng số $w_{ij}$ được gán cho từ $t_i$ của văn bản dj thuộc $D$ xác định mức quan trọng của $t_i$ trong văn bản $d_j$. Từ không xuất hiện trong $d_j$ có $w_{ij}=0$.\n",
        "- Mỗi văn bản $d_j$ được thể hiện dưới dạng vector $\\mathbf{d_j}= [w_{1j},w_{2j},...,w_{\\vert V \\vert j}]$\n",
        "- Thể hiện dữ liệu bằng một ma trận M kích thước $n \\times \\vert V \\vert$ => mỗi hàng thể hiện một văn bản."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Cài đặt Lab 2\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "import random\n",
        "import urllib.robotparser\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "# from num2words import num2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install num2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_urls(url):\n",
        "    r = requests.get(url)\n",
        "    # TODO\n",
        "    # Lấy các url nằm trong trang web của url này, lưu lại vào biến urls\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    urls = []\n",
        "    for link in soup.find_all('a', href= re.compile('http[s]?://(?:[-\\w.])+')): \n",
        "      urls.append(link.get('href'))\n",
        "\n",
        "    return urls\n",
        "\n",
        "def get_urls_recursive(start_url, limit):\n",
        "    urls = [start_url]\n",
        "    \n",
        "        # TODO\n",
        "        # Lấy các url nằm trong trang web của url này, lưu lại vào biến new_urls\n",
        "        # Với mỗi url mới trong new_urls:\n",
        "        #   Nếu nó chưa nằm trong urls thì thêm nó vô  \n",
        "        # Nếu kích thước của urls vượt quá limit thì dừng và xóa phần dư thừa\n",
        "    \n",
        "    for url in urls:\n",
        "      \n",
        "      \n",
        "      new_url = get_urls(url)\n",
        "\n",
        "      for val in new_url:\n",
        "        if val not in urls:\n",
        "          urls.append(val)\n",
        "      if len(urls) > limit:\n",
        "        urls = urls[:limit]\n",
        "        break\n",
        "    return urls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# limit = 200\n",
        "limit = 60\n",
        "url_list = get_urls_recursive('https://en.wikipedia.org/wiki/Web_mining', limit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_filter(element):\n",
        "    # TODO\n",
        "    # Cài đặt lại như Lab02\n",
        "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
        "        return False\n",
        "    elif isinstance(element, Comment):\n",
        "        '''Opinion mining?'''\n",
        "        return False\n",
        "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
        "        '''space, return, endline'''\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def wordList(url):\n",
        "    # TODO\n",
        "    # Cài đặt lại như Lab02\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "    text = soup.findAll(text=True)\n",
        "    filtered_text = list(filter(text_filter, text)) # list của các chuỗi\n",
        "    word_list = []\n",
        "\n",
        "    for strg in filtered_text: \n",
        "      new_strg = strg.replace(string.punctuation, \" \")\n",
        "      words = new_strg.split()\n",
        "      word_list.extend(words)\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def read_url(url, url_idx, data):\n",
        "    # TODO\n",
        "    # Cài đặt lại như Lab02\n",
        "    word_list = wordList(url)\n",
        "\n",
        "    for word in word_list:\n",
        "      if word not in data:\n",
        "        data[word] = [[url_idx], 1]\n",
        "      else:\n",
        "        if url_idx not in data[word][0]:\n",
        "          data[word][0].append(url_idx)\n",
        "        data[word][1]+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {}\n",
        "for url_index, url in enumerate(url_list, 1):\n",
        "    read_url(url, url_index, data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tiền xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "english_stopwords = stopwords.words('english')\n",
        "print(english_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Loại bỏ các key của biến data mà nằm trong danh sách english_stopwords\n",
        "for val in list(data):\n",
        "  if val in english_stopwords:\n",
        "    del data[val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# chuyển đổi dữ liệu về chữ thường và gộp các key trùng\n",
        "data = {key.lower():val for key, val in data.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# loại bỏ các từ không phải tiếng anh\n",
        "for key in data:\n",
        "    if not key.isascii():\n",
        "        del data[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO\n",
        "# chuyển đổi các từ về dạng nguyên gốc và gộp các key trùng\n",
        "# VD: created => create, creating => create\n",
        "# Gợi ý: sử dụng nltk.stem và PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "data2 = {}\n",
        "\n",
        "for key, val in data.items():\n",
        "  stemmed_key = ps.stem(key)\n",
        "  data2[stemmed_key] = val\n",
        "\n",
        "# TODO\n",
        "# chuyển đổi các số về dạng chữ\n",
        "# VD: 123 => one two three\n",
        "# Gợi ý: sử dụng num2words\n",
        "for key in list(data2):\n",
        "    if key not in data2:\n",
        "        continue\n",
        "    if key.isdigit():\n",
        "        data2[num2words(key)] = data2.pop(key)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
        "TF-IDF cho biết độ quan trọng của một từ đối với một tài liệu trong ngữ liệu.\n",
        "\n",
        "TF- Term Frequency : dùng để ước lượng tần xuất xuất hiện của từ trong văn bản. Tuy nhiên với mỗi văn bản thì có độ dài khác nhau số lần xuất hiện của từ có thể nhiều hơn . Vì vậy số lần xuất hiện của từ sẽ được chuẩn hóa bằng cách chia cho độ dài của văn bản (tổng số từ trong văn bản đó).\n",
        "    \n",
        "$$tf_{t}=\\dfrac{f(t,d)}{\\sum_{i \\in d}f(i,d)}$$ \n",
        "\n",
        "- $f(t,d)$ là số lần xuất hiện từ $t$ trong văn bản $d$.\n",
        "\n",
        "IDF- Inverse Document Frequency: Khi tính tần số xuất hiện TF thì các từ đều được coi là quan trọng như nhau. Tuy nhiên có một số từ thường được được sử dụng nhiều nhưng không quan trọng để thể hiện ý nghĩa của đoạn văn như \"is\", \"the\"... (các từ chức năng). Ta cần giảm độ quan trọng của những từ này.\n",
        "\n",
        "$$idf_t=\\log \\left(\\dfrac{n}{df_t}\\right)$$\n",
        "\n",
        "- *n* là số văn bản.\n",
        "\n",
        "- $df_t$ là số văn bản xuất hiện từ t\n",
        "\n",
        "**TF-IDF:** $$tf_{t} \\times idf_t$$\n",
        "\n",
        "- $tf_{t}$ càng lớn tần suất xuất hiện của từ trong văn bản càng lớn.\n",
        "- $idf_t$ càng lớn từ hiếm khi xuất hiện trong tập dữ liệu.\n",
        "- **Giả định** những đặc trưng quan trọng nhất là những đặc trưng hiếm xuất hiện."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO\n",
        "def cal_tf_idf(data, n_doc):\n",
        "    \n",
        "    n_words = len(data)\n",
        "    M = np.zeros(shape=(n_doc,n_words),dtype=float)\n",
        "   \n",
        "    for i, (key, value) in enumerate(data.items()):\n",
        "        for j in range(n_doc):\n",
        "            M[j][i] = (value[1]/n_words) * np.log(n_doc/len(value[0]))\n",
        "\n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fapsiwDoc9e8"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "n_urls=60\n",
        "\n",
        "M = cal_tf_idf(data, n_urls)\n",
        "\n",
        "#TODO save data M to txt file using numpy\n",
        "np.savetxt('result.txt', M, fmt='%f')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Truy vấn thông tin"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Công thức tính Cosine Similarity\n",
        "\n",
        "Tính toán độ tương tự của truy vấn 𝐪 với\n",
        "mỗi tài liệu $𝐝_𝑗$ trong tập hợp tài liệu 𝐷\n",
        "\n",
        "![cosine](cosine.PNG)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO\n",
        "def cal_cosine_similarity(d, q):\n",
        "    #tính toán giá trị Cosine similarity với từng tài liệu dj trong tập hợp tài liệu D \n",
        "    dot_product = np.dot(d, q)\n",
        "\n",
        "    # Compute the L2 norm of d and q\n",
        "    norm_d = np.linalg.norm(d)\n",
        "    norm_q = np.linalg.norm(q)\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    cosine_similarity = dot_product / (norm_d * norm_q)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def information_retrieval(q):\n",
        "    # q: query\n",
        "    # trả về đường dẫn có trang chứa giá trị Cosine similarity cao nhất với câu query\n",
        "    cosine_similarities = []\n",
        "    for w in M:\n",
        "        cosine_similarities.append(cal_cosine_similarity(w, q))\n",
        "    \n",
        "    #TODO\n",
        "\n",
        "\n",
        "    return {\"url\": link, \"cosine_similarity\": cosine_similarity}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(information_retrieval(\"Web mining\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab05-TF-IDF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
