{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QACY3c_lr4uz"
      },
      "source": [
        "# Lab03: TF-IDF.\n",
        "\n",
        "- MSSV: 19120689\n",
        "- H·ªç v√† t√™n: L·∫°i Kh√°nh To√†n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eD8WblEMr4u1"
      },
      "source": [
        "## Y√™u c·∫ßu b√†i t·∫≠p\n",
        "\n",
        "**C√°ch l√†m b√†i**\n",
        "\n",
        "\n",
        "B·∫°n s·∫Ω l√†m tr·ª±c ti·∫øp tr√™n file notebook n√†y; t·ª´ `TODO` cho bi·∫øt nh·ªØng ph·∫ßn m√† b·∫°n c·∫ßn ph·∫£i l√†m.\n",
        "\n",
        "B·∫°n c√≥ th·ªÉ th·∫£o lu·∫≠n √Ω t∆∞·ªüng c≈©ng nh∆∞ tham kh·∫£o c√°c t√†i li·ªáu, nh∆∞ng *code v√† b√†i l√†m ph·∫£i l√† c·ªßa b·∫°n*. \n",
        "\n",
        "N·∫øu vi ph·∫°m th√¨ s·∫Ω b·ªã 0 ƒëi·ªÉm cho b√†i t·∫≠p n√†y.\n",
        "\n",
        "**C√°ch n·ªôp b√†i**\n",
        "\n",
        "Tr∆∞·ªõc khi n·ªôp b√†i, rerun l·∫°i notebook (`Kernel` -> `Restart & Run All`).\n",
        "\n",
        "Sau ƒë√≥, t·∫°o th∆∞ m·ª•c c√≥ t√™n `MSSV` c·ªßa b·∫°n (vd, n·∫øu b·∫°n c√≥ MSSV l√† 1234567 th√¨ b·∫°n ƒë·∫∑t t√™n th∆∞ m·ª•c l√† `1234567`) Ch√©p file notebook, file `tf_idf_data.txt` c·ªßa c√°c b·∫°n (n·∫øu file n√†y k√≠ch th∆∞·ªõc l·ªõn c√°c b·∫°n c√≥ th·ªÉ ch√©p link v√†o `link_data.txt`), n√©n th∆∞ m·ª•c `MSSV` n√†y l·∫°i v√† n·ªôp tr√™n moodle.\n",
        "\n",
        "**N·ªôi dung b√†i t·∫≠p**\n",
        "\n",
        "C√†i ƒë·∫∑t m·ªôt web crawler ƒë·ªÉ thu th·∫≠p v√† bi·ªÉu di·ªÖn d·ªØ li·ªáu b·∫±ng kh√¥ng gian vector v√† tr·ªçng s·ªë TF-IDF: https://en.wikipedia.org/wiki/Web_mining. Truy v·∫•n d·ª±a tr√™n cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txFiZE0Zr4u2"
      },
      "source": [
        "## N·ªôi dung b√†i t·∫≠p"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pU505-86r4u4"
      },
      "source": [
        "## 1. Kh√¥ng gian vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DCPXdVrr4u4"
      },
      "source": [
        "- M·ªôt vector 2 chi·ªÅu c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt d∆∞·ªõi d·∫°ng `[x,y]`\n",
        "- M·ªôt vector 3 chi·ªÅu c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt d∆∞·ªõi d·∫°ng `[x,y,z]`\n",
        "![vector](vector.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tFgEzX5Xc9e6"
      },
      "source": [
        "- ƒê·∫∑t $V$ l√† t·∫≠p h·ª£p c√°c ƒë·∫∑c tr∆∞ng trong th·ªÉ hi·ªán d·ªØ li·ªáu.\n",
        "- B·∫•t k·ª≥ m·∫´u d·ªØ li·ªáu n√†o ƒë·ªÅu c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt vect∆° v·ªõi $\\vert V\\vert$ chi·ªÅu\n",
        "- V√≠ d·ª•: gi·∫£ s·ª≠ ch√∫ng ta c√≥ 3 ƒë·∫∑c tr∆∞ng l√† c√°c t·ª´ dog, bite, man. Gi√° tr·ªã 1 th·ªÉ hi·ªán t·ª´ ƒë√≥ xu·∫•t hi·ªán 1 l·∫ßn, 0 l√† kh√¥ng xu·∫•t hi·ªán.  \n",
        "![space](space.png)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6kgzYNRKr4u5"
      },
      "source": [
        "## 2. Thu th·∫≠p v√† th·ªÉ hi·ªán d·ªØ li·ªáu\n",
        "Thu th·∫≠p d·ªØ li·ªáu b·∫Øt ƒë·∫ßu t·ª´ URL: https://en.wikipedia.org/wiki/Web_mining.\n",
        "- G·ªçi $D$ l√† m·ªôt t·∫≠p vƒÉn b·∫£n ch·ª©a *n* vƒÉn b·∫£n: $D=\\left\\{d_1,d_2,...,d_n\\right\\}$.\n",
        "- $V=\\left\\{v_1,v_2,...v_{\\vert V \\vert}\\right\\}$ l√† t·ª´ ƒëi·ªÉn (t·∫•t c·∫£ c√°c t·ª´ ph√¢n bi·ªát trong d·ªØ li·ªáu thu ƒë∆∞·ª£c). $\\vert V \\vert$ l√† k√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn.\n",
        "- M·ªôt tr·ªçng s·ªë $w_{ij}$ ƒë∆∞·ª£c g√°n cho t·ª´ $t_i$ c·ªßa vƒÉn b·∫£n dj thu·ªôc $D$ x√°c ƒë·ªãnh m·ª©c quan tr·ªçng c·ªßa $t_i$ trong vƒÉn b·∫£n $d_j$. T·ª´ kh√¥ng xu·∫•t hi·ªán trong $d_j$ c√≥ $w_{ij}=0$.\n",
        "- M·ªói vƒÉn b·∫£n $d_j$ ƒë∆∞·ª£c th·ªÉ hi·ªán d∆∞·ªõi d·∫°ng vector $\\mathbf{d_j}= [w_{1j},w_{2j},...,w_{\\vert V \\vert j}]$\n",
        "- Th·ªÉ hi·ªán d·ªØ li·ªáu b·∫±ng m·ªôt ma tr·∫≠n M k√≠ch th∆∞·ªõc $n \\times \\vert V \\vert$ => m·ªói h√†ng th·ªÉ hi·ªán m·ªôt vƒÉn b·∫£n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#C√†i ƒë·∫∑t Lab 2\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "import random\n",
        "import urllib.robotparser\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "# from num2words import num2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install num2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_urls(url):\n",
        "    r = requests.get(url)\n",
        "    # TODO\n",
        "    # L·∫•y c√°c url n·∫±m trong trang web c·ªßa url n√†y, l∆∞u l·∫°i v√†o bi·∫øn urls\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    urls = []\n",
        "    for link in soup.find_all('a', href= re.compile('http[s]?://(?:[-\\w.])+')): \n",
        "      urls.append(link.get('href'))\n",
        "\n",
        "    return urls\n",
        "\n",
        "def get_urls_recursive(start_url, limit):\n",
        "    urls = [start_url]\n",
        "    \n",
        "        # TODO\n",
        "        # L·∫•y c√°c url n·∫±m trong trang web c·ªßa url n√†y, l∆∞u l·∫°i v√†o bi·∫øn new_urls\n",
        "        # V·ªõi m·ªói url m·ªõi trong new_urls:\n",
        "        #   N·∫øu n√≥ ch∆∞a n·∫±m trong urls th√¨ th√™m n√≥ v√¥  \n",
        "        # N·∫øu k√≠ch th∆∞·ªõc c·ªßa urls v∆∞·ª£t qu√° limit th√¨ d·ª´ng v√† x√≥a ph·∫ßn d∆∞ th·ª´a\n",
        "    \n",
        "    for url in urls:\n",
        "      \n",
        "      \n",
        "      new_url = get_urls(url)\n",
        "\n",
        "      for val in new_url:\n",
        "        if val not in urls:\n",
        "          urls.append(val)\n",
        "      if len(urls) > limit:\n",
        "        urls = urls[:limit]\n",
        "        break\n",
        "    return urls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# limit = 200\n",
        "limit = 60\n",
        "url_list = get_urls_recursive('https://en.wikipedia.org/wiki/Web_mining', limit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_filter(element):\n",
        "    # TODO\n",
        "    # C√†i ƒë·∫∑t l·∫°i nh∆∞ Lab02\n",
        "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
        "        return False\n",
        "    elif isinstance(element, Comment):\n",
        "        '''Opinion mining?'''\n",
        "        return False\n",
        "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
        "        '''space, return, endline'''\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def wordList(url):\n",
        "    # TODO\n",
        "    # C√†i ƒë·∫∑t l·∫°i nh∆∞ Lab02\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "    text = soup.findAll(text=True)\n",
        "    filtered_text = list(filter(text_filter, text)) # list c·ªßa c√°c chu·ªói\n",
        "    word_list = []\n",
        "\n",
        "    for strg in filtered_text: \n",
        "      new_strg = strg.replace(string.punctuation, \" \")\n",
        "      words = new_strg.split()\n",
        "      word_list.extend(words)\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def read_url(url, url_idx, data):\n",
        "    # TODO\n",
        "    # C√†i ƒë·∫∑t l·∫°i nh∆∞ Lab02\n",
        "    word_list = wordList(url)\n",
        "\n",
        "    for word in word_list:\n",
        "      if word not in data:\n",
        "        data[word] = [[url_idx], 1]\n",
        "      else:\n",
        "        if url_idx not in data[word][0]:\n",
        "          data[word][0].append(url_idx)\n",
        "        data[word][1]+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {}\n",
        "for url_index, url in enumerate(url_list, 1):\n",
        "    read_url(url, url_index, data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "english_stopwords = stopwords.words('english')\n",
        "print(english_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Lo·∫°i b·ªè c√°c key c·ªßa bi·∫øn data m√† n·∫±m trong danh s√°ch english_stopwords\n",
        "for val in list(data):\n",
        "  if val in english_stopwords:\n",
        "    del data[val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu v·ªÅ ch·ªØ th∆∞·ªùng v√† g·ªôp c√°c key tr√πng\n",
        "data = {key.lower():val for key, val in data.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# lo·∫°i b·ªè c√°c t·ª´ kh√¥ng ph·∫£i ti·∫øng anh\n",
        "for key in data:\n",
        "    if not key.isascii():\n",
        "        del data[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO\n",
        "# chuy·ªÉn ƒë·ªïi c√°c t·ª´ v·ªÅ d·∫°ng nguy√™n g·ªëc v√† g·ªôp c√°c key tr√πng\n",
        "# VD: created => create, creating => create\n",
        "# G·ª£i √Ω: s·ª≠ d·ª•ng nltk.stem v√† PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "data2 = {}\n",
        "\n",
        "for key, val in data.items():\n",
        "  stemmed_key = ps.stem(key)\n",
        "  data2[stemmed_key] = val\n",
        "\n",
        "# TODO\n",
        "# chuy·ªÉn ƒë·ªïi c√°c s·ªë v·ªÅ d·∫°ng ch·ªØ\n",
        "# VD: 123 => one two three\n",
        "# G·ª£i √Ω: s·ª≠ d·ª•ng num2words\n",
        "for key in list(data2):\n",
        "    if key not in data2:\n",
        "        continue\n",
        "    if key.isdigit():\n",
        "        data2[num2words(key)] = data2.pop(key)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
        "TF-IDF cho bi·∫øt ƒë·ªô quan tr·ªçng c·ªßa m·ªôt t·ª´ ƒë·ªëi v·ªõi m·ªôt t√†i li·ªáu trong ng·ªØ li·ªáu.\n",
        "\n",
        "TF- Term Frequency : d√πng ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng t·∫ßn xu·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ trong vƒÉn b·∫£n. Tuy nhi√™n v·ªõi m·ªói vƒÉn b·∫£n th√¨ c√≥ ƒë·ªô d√†i kh√°c nhau s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ c√≥ th·ªÉ nhi·ªÅu h∆°n . V√¨ v·∫≠y s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ s·∫Ω ƒë∆∞·ª£c chu·∫©n h√≥a b·∫±ng c√°ch chia cho ƒë·ªô d√†i c·ªßa vƒÉn b·∫£n (t·ªïng s·ªë t·ª´ trong vƒÉn b·∫£n ƒë√≥).\n",
        "    \n",
        "$$tf_{t}=\\dfrac{f(t,d)}{\\sum_{i \\in d}f(i,d)}$$ \n",
        "\n",
        "- $f(t,d)$ l√† s·ªë l·∫ßn xu·∫•t hi·ªán t·ª´ $t$ trong vƒÉn b·∫£n $d$.\n",
        "\n",
        "IDF- Inverse Document Frequency: Khi t√≠nh t·∫ßn s·ªë xu·∫•t hi·ªán TF th√¨ c√°c t·ª´ ƒë·ªÅu ƒë∆∞·ª£c coi l√† quan tr·ªçng nh∆∞ nhau. Tuy nhi√™n c√≥ m·ªôt s·ªë t·ª´ th∆∞·ªùng ƒë∆∞·ª£c ƒë∆∞·ª£c s·ª≠ d·ª•ng nhi·ªÅu nh∆∞ng kh√¥ng quan tr·ªçng ƒë·ªÉ th·ªÉ hi·ªán √Ω nghƒ©a c·ªßa ƒëo·∫°n vƒÉn nh∆∞ \"is\", \"the\"... (c√°c t·ª´ ch·ª©c nƒÉng). Ta c·∫ßn gi·∫£m ƒë·ªô quan tr·ªçng c·ªßa nh·ªØng t·ª´ n√†y.\n",
        "\n",
        "$$idf_t=\\log \\left(\\dfrac{n}{df_t}\\right)$$\n",
        "\n",
        "- *n* l√† s·ªë vƒÉn b·∫£n.\n",
        "\n",
        "- $df_t$ l√† s·ªë vƒÉn b·∫£n xu·∫•t hi·ªán t·ª´ t\n",
        "\n",
        "**TF-IDF:** $$tf_{t} \\times idf_t$$\n",
        "\n",
        "- $tf_{t}$ c√†ng l·ªõn t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ trong vƒÉn b·∫£n c√†ng l·ªõn.\n",
        "- $idf_t$ c√†ng l·ªõn t·ª´ hi·∫øm khi xu·∫•t hi·ªán trong t·∫≠p d·ªØ li·ªáu.\n",
        "- **Gi·∫£ ƒë·ªãnh** nh·ªØng ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t l√† nh·ªØng ƒë·∫∑c tr∆∞ng hi·∫øm xu·∫•t hi·ªán."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO\n",
        "def cal_tf_idf(data, n_doc):\n",
        "    \n",
        "    n_words = len(data)\n",
        "    M = np.zeros(shape=(n_doc,n_words),dtype=float)\n",
        "   \n",
        "    for i, (key, value) in enumerate(data.items()):\n",
        "        for j in range(n_doc):\n",
        "            M[j][i] = (value[1]/n_words) * np.log(n_doc/len(value[0]))\n",
        "\n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fapsiwDoc9e8"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "n_urls=60\n",
        "\n",
        "M = cal_tf_idf(data, n_urls)\n",
        "\n",
        "#TODO save data M to txt file using numpy\n",
        "np.savetxt('result.txt', M, fmt='%f')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Truy v·∫•n th√¥ng tin"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√¥ng th·ª©c t√≠nh Cosine Similarity\n",
        "\n",
        "T√≠nh to√°n ƒë·ªô t∆∞∆°ng t·ª± c·ªßa truy v·∫•n ùê™ v·ªõi\n",
        "m·ªói t√†i li·ªáu $ùêù_ùëó$ trong t·∫≠p h·ª£p t√†i li·ªáu ùê∑\n",
        "\n",
        "![cosine](cosine.PNG)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO\n",
        "def cal_cosine_similarity(d, q):\n",
        "    #t√≠nh to√°n gi√° tr·ªã Cosine similarity v·ªõi t·ª´ng t√†i li·ªáu dj trong t·∫≠p h·ª£p t√†i li·ªáu D \n",
        "    dot_product = np.dot(d, q)\n",
        "\n",
        "    # Compute the L2 norm of d and q\n",
        "    norm_d = np.linalg.norm(d)\n",
        "    norm_q = np.linalg.norm(q)\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    cosine_similarity = dot_product / (norm_d * norm_q)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def information_retrieval(q):\n",
        "    # q: query\n",
        "    # tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n c√≥ trang ch·ª©a gi√° tr·ªã Cosine similarity cao nh·∫•t v·ªõi c√¢u query\n",
        "    cosine_similarities = []\n",
        "    for w in M:\n",
        "        cosine_similarities.append(cal_cosine_similarity(w, q))\n",
        "    \n",
        "    #TODO\n",
        "\n",
        "\n",
        "    return {\"url\": link, \"cosine_similarity\": cosine_similarity}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(information_retrieval(\"Web mining\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab05-TF-IDF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
